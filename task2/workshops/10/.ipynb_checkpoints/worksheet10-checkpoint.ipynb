{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP90051 Workshop 10\n",
    "## Principal Component Analysis\n",
    "***\n",
    "In this worksheet we explore several aspects of Principal Component Analysis (PCA).\n",
    "1. We implement PCA using singular value decomposition (SVD)\n",
    "2. We apply PCA to visualise high-dimensional data in 3D\n",
    "3. We interpret PCA as a data compression algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D # enables 3D plotting\n",
    "\n",
    "from sklearn.datasets import make_low_rank_matrix\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. PCA using SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by generating some synthetic data to demonstrate PCA.\n",
    "We'll work in 3D so that we can visualise the data, and later on, the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = make_low_rank_matrix(n_features=3, effective_rank=1, n_samples=500, tail_strength=10, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block below demonstrates the 3D plotting functionality in `matplotlib`.\n",
    "You can use the left mouse button to rotate the axes, and the right mouse button to zoom.\n",
    "\n",
    "*Note: we must explicitly initialise a figure object to start a new plot when using the interactive plotting library.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:,0], X[:,1], X[:,2])\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Is there a direction along which the data has lower variance? You'll likely need to rotate the figure to answer this question.\n",
    "\n",
    "Let's now apply PCA.\n",
    "\n",
    "#### Step A: Centring the features\n",
    "To ensure the first principal component describes the direction of maxmimum variance, the data matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times m}$ must be centred. \n",
    "In other words, each column should have zero mean.\n",
    "Complete the code block below, storing the centred data matrix in `X_centered`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = ... # fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step B: Solving the eigensystem\n",
    "In lectures, it was shown that the principal components $\\{\\mathbf{p}_1, \\ldots, \\mathbf{p}_m\\}$ of $\\mathbf{X}$ are the eigenvectors of the data covariance matrix \n",
    "$$\n",
    "\\mathbf{\\Sigma} = \\frac{1}{n - 1} \\sum_{i = 1}^{n} \\mathbf{x}_i^\\intercal \\mathbf{x}_i = \\frac{1}{n - 1} \\mathbf{X}^\\intercal \\mathbf{X}\n",
    "$$\n",
    "(assuming $\\mathbf{X}$ is centred).\n",
    "The eigenvalues $\\lambda_i$ may be interpreted as the variance explained by component $\\mathbf{p}_i$.\n",
    "\n",
    "We can obtain the eigenvalues/eigenvectors of $\\mathbf{\\Sigma}$ using the singular value decomposition (SVD) of $\\mathbf{X}$.\n",
    "Specifically, we write $\\mathbf{X}$ as $\\mathbf{U} \\mathbf{S} \\mathbf{V}^\\intercal$, where\n",
    "* $\\mathbf{S}$ is a $n \\times m$ rectangular diagonal matrix containing the singular values of $\\mathbf{X}$\n",
    "* $\\mathbf{U}$ is an $n \\times n$ unitary matrix whose columns are the left singular vectors of $\\mathbf{X}$\n",
    "* $\\mathbf{V}$ is a $m \\times m$ unitary matrix whose columns are the right singular vectors of $\\mathbf{X}$\n",
    "\n",
    "Then, observing that $\\mathbf{\\Sigma} = \\frac{1}{n-1} \\mathbf{V} \\mathbf{S}^\\intercal \\mathbf{S} \\mathbf{V}^\\intercal$ we have:\n",
    "* the eigenvalues of $\\mathbf{\\Sigma}$ are the *squared* singular values of $\\mathbf{X}$, scaled by $\\frac{1}{n-1}$\n",
    "* the eigenvectors of $\\mathbf{\\Sigma}$ are the columns of $\\mathbf{V}$.\n",
    "\n",
    "Use this information to compute \n",
    "$$\n",
    "\\texttt{eVecs} = \\begin{bmatrix}\n",
    "\\mathbf{p}_1 \\\\\n",
    "\\mathbf{p}_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{p}_m\n",
    "\\end{bmatrix} \\ \\text{ and } \n",
    "\\texttt{eVals} = \\begin{bmatrix}\n",
    "\\lambda_1, \\lambda_2, \\cdots, \\lambda_m\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "in the code block below. \n",
    "*(Hint: use the `numpy.linalg.svd` function.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in below\n",
    "...\n",
    "...\n",
    "...\n",
    "\n",
    "for eVal, eVec in zip(eVals, eVecs):\n",
    "    print('Eigenvalue {:.3g} with eigenvector {}'.format(eVal, eVec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've computed the principal components, let's plot them along with the data.\n",
    "The `quiver` plotting function allows us to plot arrows in 3D.\n",
    "We must specify the origins of the arrows (`x`, `y`, `z` below) and the end points of the arrows (`dx`, `dy`, `dz` below).\n",
    "Note: we also rescale the eigenvectors so they're roughly the same scale as the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = np.zeros_like(eVecs)\n",
    "dx, dy, dz = eVecs.T\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_centered[:,0], X_centered[:,1], X_centered[:,2])\n",
    "ax.quiver(x, y, z, dx, dy, dz, colors='r')\n",
    "ax.axis('equal')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we decide to represent the data using only the first and second principal components $\\mathbf{p}_1$ and $\\mathbf{p}_2$.\n",
    "We can project the data onto the space spanned by $\\mathbf{p}_1, \\mathbf{p}_2$ as follows:\n",
    "$$\n",
    "\\mathbf{X}_{\\mathrm{pca}} = \\mathbf{X} \\begin{bmatrix}\n",
    "    \\mathbf{p}_1^\\intercal, \\mathbf{p}_2^\\intercal\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Use this to equation to compute `X_pca` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = ... # fill in\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(X_pca[:,0], X_pca[:,1])\n",
    "plt.xlabel('$p_1$')\n",
    "plt.ylabel('$p_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Apply the inverse transform to map `X_pca` back to the original 3D space and plot the data alongside `X`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PCA for visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now apply PCA to a real data set with many features.\n",
    "We'll use the `digits` data set available in scikit-learn.\n",
    "It contains 1797 8Ã—8 grayscale images of digits (0-9), along with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "Y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we plot a random subset of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = 6\n",
    "rows = 6\n",
    "image_ids = np.random.choice(X.shape[0], size = rows*columns, replace=False)\n",
    "fig = plt.figure()\n",
    "for i,image_id in enumerate(image_ids):\n",
    "    ax = fig.add_subplot(rows, columns, i + 1)\n",
    "    ax.imshow(X[image_id].reshape(8,-1), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply PCA to visualise the data in 3D.\n",
    "Notice that the instances are clustered according to the digit (as represented by the colour)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_pca = PCA(n_components=3).fit(X)\n",
    "X_pca = digits_pca.transform(X)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for digit in range(10):\n",
    "    ax.scatter(X_pca[Y==digit,0], X_pca[Y==digit,1], X_pca[Y==digit,2], label=digit)\n",
    "ax.set_xlabel('$p_1$')\n",
    "ax.set_ylabel('$p_2$')\n",
    "ax.set_zlabel('$p_3$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Repeat this visualisation exercise in 2D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. PCA for data compression\n",
    "Recall that PCA can be interpeted as finding a set of basis vectors $\\{\\mathbf{p}_1, \\ldots, \\mathbf{p}_k\\}$ that minimise the residual sum-squared reconstruction error.\n",
    "\n",
    "Specifically, we can express the original feature vector $\\mathbf{x} \\in \\mathbb{R}^{m}$ in the lower dimensional feature space $\\mathbb{R}^k$ as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{\\mathrm{pca}} = (\\mathbf{x} \\cdot \\mathbf{p}_1) \\mathbf{p}_1 +  \\cdots + (\\mathbf{x} \\cdot \\mathbf{p}_k) \\mathbf{p}_k = \\mathbf{x} \\mathbf{W}\n",
    "$$\n",
    "where $\\mathbf{W} = \\begin{bmatrix} \\mathbf{p}_1 \\\\ \\vdots \\\\ \\mathbf{p}_k \\end{bmatrix}^\\intercal$.\n",
    "The reconstructed feature vector is then $\\mathbf{x}_{\\mathrm{recon}} = \\mathbf{x} \\mathbf{W} \\mathbf{W}^\\intercal$.\n",
    "\n",
    "Below, we visualise some of the reconstructed images alongside the originals.\n",
    "Note that `sklearn.decomposition.PCA` has a method for applying the inverse transform $\\mathbf{W}^\\intercal$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "image_ids = np.random.choice(X.shape[0], size=n, replace=False)\n",
    "original = X[image_ids].reshape(n, 8, -1)\n",
    "labels = Y[image_ids]\n",
    "reconstructed = digits_pca.inverse_transform(X_pca[image_ids]).reshape(n, 8, -1)\n",
    "\n",
    "fig = plt.figure(figsize=(4,8))\n",
    "for i in range(n):\n",
    "    ax = fig.add_subplot(n, 2, 2*i + 1)\n",
    "    ax.imshow(original[i], cmap='binary')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Original {}'.format(labels[i]))\n",
    "    ax = fig.add_subplot(n, 2, 2*i + 2)\n",
    "    ax.imshow(reconstructed[i], cmap='binary')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Reconstructed')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Try compressing the images with a larger number of principal components (> 3).\n",
    "\n",
    "**Exercise:** Make a Scree plot for the digits data set (proportion of explained variance plotted against number of components)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
